{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38ac09e9",
   "metadata": {},
   "source": [
    "# üóìÔ∏è W02 - NB01 | Lecture Demo: Wikipedia Scraping with Scrapy\n",
    "\n",
    "**DS205 W02 NB01 ‚Äì Advanced Data Manipulation (Winter Term 2025/2026)**\n",
    "\n",
    "<div style=\"font-family: system-ui; padding: 20px 30px 20px 20px; background-color: #FFFFFF; border-left: 8px solid #ED9255; border-radius: 8px; box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);max-width:600px;color:#212121;\">\n",
    "\n",
    "**Lecture Demonstration Notebook**\n",
    "- üìÖ Date: 26 January 2026\n",
    "- üë§ Instructor: Dr Jon Cardoso-Silva\n",
    "- üéØ Purpose: Demonstrate how to inspect HTML and build selectors before writing a spider\n",
    "\n",
    "ü•Ö **Learning Goals**\n",
    "\n",
    "<ul style=\"margin: 0.2em 0 0.4em 0; padding-left: 1.25em; font-size:1em; list-style-type:none;font-size:0.85em;color:#666666\">\n",
    "\n",
    "  <li style=\"margin-bottom:0.15em; padding-left:0.4em; text-indent:-0.4em;\">\n",
    "    <span style=\"display:inline-block;font-weight:450;width:0.75em\">i)</span> Send a simple HTTP request and inspect the response,\n",
    "  </li>\n",
    "  <li style=\"margin-bottom:0.15em; padding-left:0.4em; text-indent:-0.4em;\">\n",
    "    <span style=\"display:inline-block;font-weight:450;width:0.75em\">ii)</span> Build selectors step by step until they are precise,\n",
    "  </li>\n",
    "  <li style=\"margin-bottom:0.15em; padding-left:0.4em; text-indent:-0.4em;\">\n",
    "    <span style=\"display:inline-block;font-weight:450;width:0.75em\">iii)</span> Extract a table with `read_html` and a manual fallback,\n",
    "  </li>\n",
    "  <li style=\"padding-left:0.4em; text-indent:-0.4em;\">\n",
    "    <span style=\"display:inline-block;font-weight:450;width:0.75em\">iv)</span> Move notebook logic into a Scrapy spider.\n",
    "  </li>\n",
    "</ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a2ff16",
   "metadata": {},
   "source": [
    "## Reference Links\n",
    "\n",
    "You will use these frequently:\n",
    "\n",
    "- [Scrapy selectors overview](https://docs.scrapy.org/en/latest/topics/selectors.html#topics-selectors)\n",
    "- [Scrapy CSS selector extensions (`::text`, `::attr()`)](https://docs.scrapy.org/en/latest/topics/selectors.html#extensions-to-css-selectors)\n",
    "- [Scrapy quick overview (runspider flow)](https://docs.scrapy.org/en/latest/intro/overview.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab233493",
   "metadata": {},
   "source": [
    "## Environment Setup (Conda)\n",
    "\n",
    "For this lecture, use a dedicated conda environment called `food`. The `environment.yml` file lives next to this notebook.\n",
    "\n",
    "```bash\n",
    "conda env create -f environment.yml\n",
    "conda activate food\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a26c3ad",
   "metadata": {},
   "source": [
    "‚öôÔ∏è **Importing libraries**\n",
    "\n",
    "Here are the libraries we are using today:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df9faeb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from io import StringIO\n",
    "\n",
    "from scrapy.selector import Selector\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython.display import display\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"Consider using IPython.display.IFrame instead\", category=UserWarning, module=\"IPython.core.display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4617311e",
   "metadata": {},
   "source": [
    "## Section 1: A Simple Request and Its Response\n",
    "\n",
    "We will send one request and look at the response. This keeps the focus on HTML inspection.\n",
    "\n",
    "**What is a User-Agent?**\n",
    "\n",
    "A User-Agent is a short text string that tells the server what kind of client is making the request.\n",
    "Some websites block unknown or empty User-Agent headers, so we set a clear one.\n",
    "\n",
    "See the [Wikimedia policy](https://foundation.wikimedia.org/wiki/Policy:Wikimedia_Foundation_User-Agent_Policy) to understand why I'm identifying myself as a bot in this particular way in the UserAgent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df216d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_URL = \"https://en.wikipedia.org/wiki/List_of_foods\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": (\n",
    "        \"DS205W02LectureBot/1.0 \"\n",
    "        \"(https://lse-dsi.github.io/DS205/2025-2026/winter-term/; \"\n",
    "        \"J.Cardoso-Silva@lse.ac.uk) \"\n",
    "        \"requests/2.x\"\n",
    "    )\n",
    "}\n",
    "output = requests.get(LIST_URL, headers=headers, timeout=30)\n",
    "\n",
    "print(f\"Status code: {output.status_code}\")\n",
    "print(f\"Content length: {len(output.text)}\")\n",
    "print(output.content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6646fc1c",
   "metadata": {},
   "source": [
    "üìã **Take Note:** \n",
    "\n",
    "Our response had a status code of [HTTP 200](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status) which means `OK` (success) - it worked!\n",
    "\n",
    "But notice that the content we get back (`response.content`) does NOT look like a JSON. The output a request sent to a regular website is also plain text but unlike with APIs (as we saw in last week's notebook), the data doesn't come formatted as JSON but rather as [HTML](https://developer.mozilla.org/en-US/docs/Web/HTML) (**H**yper**T**ext **M**arkup **L**anguage).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6a185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want you can look at the HTML in full by saving it to a file\n",
    "# You can use VS Code capabilities to navigate the content\n",
    "with open(\"wikipedia_list_of_foods.html\", \"wb\") as f:\n",
    "    f.write(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2c44bc",
   "metadata": {},
   "source": [
    "### Meet Scrapy\n",
    "\n",
    "There are many open-source libraries written for the Python community that allows us to parse HTML, with the most commmon one being [beautifulsoup](https://beautiful-soup-4.readthedocs.io/en/latest/). While a great library, **WE WON'T USE BEAUTIFULSOUP IN THIS COURSE!**. Instead, we will work with [`scrapy`](https://www.scrapy.org/) or, if needed, [`Selenium`](https://selenium-python.readthedocs.io/). These two libraries, in particular `scrapy`, are more appropriate for when we want to write <span style=\"background-color:#0C56AA;padding:0.05em 0.2em;color:white;border-radius:0.25em;\">**production-ready**</span> code.\n",
    "\n",
    "What you need is to pass that HTML text to [scrapy's Selector](https://docs.scrapy.org/en/latest/topics/selectors.html), which we have already imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea33f1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = Selector(text=output.content)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74848888",
   "metadata": {},
   "source": [
    "But then, to find out which information you want to collect, you need to know where the relevant data is.\n",
    "\n",
    "Here it is important to learn a bit of [CSS](https://developer.mozilla.org/en-US/docs/Web/CSS) (the **C**ascading **S**tyling **S**heets language).\n",
    "\n",
    "üìö **Recommended Reading:**\n",
    "\n",
    "* [What is CSS?](https://developer.mozilla.org/en-US/docs/Learn_web_development/Core/Styling_basics/What_is_CSS)\n",
    "* [CSS Getting Started](https://developer.mozilla.org/en-US/docs/Learn_web_development/Core/Styling_basics/Getting_started)\n",
    "* [Basic Selector](https://developer.mozilla.org/en-US/docs/Learn_web_development/Core/Styling_basics/Basic_selectors)\n",
    "* [Attribute Selector](https://developer.mozilla.org/en-US/docs/Learn_web_development/Core/Styling_basics/Attribute_selectors)\n",
    "* [Pseudo-class and elements](https://developer.mozilla.org/en-US/docs/Learn_web_development/Core/Styling_basics/Pseudo_classes_and_elements) \n",
    "\n",
    "    (*Note: most of these pseudoclasses don't work with scrapy, sadly. Check out [Scrapy selectors overview](https://docs.scrapy.org/en/latest/topics/selectors.html#topics-selectors) and [Scrapy CSS selector extensions (`::text`, `::attr()`)](https://docs.scrapy.org/en/latest/topics/selectors.html#extensions-to-css-selectors) to learn more*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cbb776",
   "metadata": {},
   "source": [
    "**To query the HTML content contained in the `html_page` object, just pass a CSS selector to it with the `.css()` method.**\n",
    "\n",
    "For example, to gather `<h1>`'s from the page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbc3222",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.css('h1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580c111a",
   "metadata": {},
   "source": [
    "‚òùÔ∏è The above shows that we received a list containing a single element, itself another object of the class Selector. If we want the _content_ of that, we should use:\n",
    "\n",
    "* `.css(..some_selector..).get()` (to get just the first element of the list) or\n",
    "* `.css(..som_selector..).getall()` (if you had multiple matches and you want to extract all)\n",
    "\n",
    "Compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93625179",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.css('h1').get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43205b6",
   "metadata": {},
   "source": [
    "with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442e27d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.css('h1').getall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2e21ab",
   "metadata": {},
   "source": [
    "üí° **The type of data returned by `.get()` is a string whereas the type returned by `.getall()` is a list of strings.**\n",
    "\n",
    "**Note also:** If `getall()` returns an empty list, your selector didn't match anything. First thing to check: does that element actually exist with the class you expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ac08c3",
   "metadata": {},
   "source": [
    "### 'Looking' at the page from the notebook\n",
    "\n",
    "If you just want to get a sense for how the HTML looks like (without all of its styling though), you can use an IFrame from the IPython library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b10d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_snippet = response.css('h1').get()\n",
    "\n",
    "display(HTML(f\"<iframe width='800' height='100' srcdoc='{html_snippet}'></iframe>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb826c69",
   "metadata": {},
   "source": [
    "### Collecting ALL matches (for example, all images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4e5eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.css('img').getall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ce17b1",
   "metadata": {},
   "source": [
    "### Collecting attributes of HTML elements\n",
    "\n",
    "While the output above is useful, you might not want to collect the entire HTML element but rather just one piece of it (for example, the link where the image is rather than the whole `<img>` element).\n",
    "\n",
    "If we want the links to where the images are stored, we first need to understand that they are inside the `src` and to collect the content of the `src`, we use the `::attr` pseudo-selector as explained in [Scrapy CSS selector extensions (`::text`, `::attr()`)](https://docs.scrapy.org/en/latest/topics/selectors.html#extensions-to-css-selectors):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e45d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.css(\"img::attr(src)\").getall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a370a289",
   "metadata": {},
   "source": [
    "ü§î **Think about it:**\n",
    "\n",
    "Why do you think most of these URLs start with a `/` rather than the usual `http://`? If you try to copy these addresses to your browser, the browser will be confused and not understand that address. Why is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c4098c",
   "metadata": {},
   "source": [
    "üéØ **ACTION POINT**\n",
    "\n",
    "**Without looking at the solution,** try to figure out how to complete the code below such that y ou display the first image that is captured in the page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6987bfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_snippet = \"\" # Replace the empty quotes with your string\n",
    "\n",
    "display(HTML(f\"<iframe width='800' height='100' srcdoc='{html_snippet}'></iframe>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d27aea",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution</summary>\n",
    "\n",
    "```python\n",
    "html_snippet = f'<img src=\"https://en.wikipedia.org{html_page.css(\"img::attr(src)\").get()}\" alt=\"\" aria-hidden=\"true\" height=\"50\" width=\"50\">'\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3972cfaa",
   "metadata": {},
   "source": [
    "## Section 2: The Same Idea in the Terminal\n",
    "\n",
    "You can test selectors in the terminal with the Scrapy shell. We are not doing it now, but I want you to know what it looks like.\n",
    "\n",
    "```bash\n",
    "scrapy shell \"https://en.wikipedia.org/wiki/List_of_foods\"\n",
    "```\n",
    "\n",
    "**Take note**\n",
    "\n",
    "The shell gives you a `response` object, which behaves like the `Selector` we used in the notebook.\n",
    "\n",
    "Try these inside the shell:\n",
    "\n",
    "```python\n",
    "response.css(\"title::text\").get()\n",
    "response.css(\"img::attr(src)\").getall()[:5]\n",
    "```\n",
    "\n",
    "**Why this matters**\n",
    "\n",
    "It is a fast way to test a selector without writing a full spider."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa93939",
   "metadata": {},
   "source": [
    "## Section 3: Narrowing Down Selectors Step by Step\n",
    "\n",
    "We will start broad, then narrow. Each step should remove noise.\n",
    "\n",
    "### Step 1: All links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170e488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_1 = response.css(\"a::attr(href)\").getall()\n",
    "print(f\"Step 1: All links: {len(step_1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc97342b",
   "metadata": {},
   "source": [
    "**Take note**\n",
    "\n",
    "This is too broad, it includes links to edit pages, menus, and other site furniture.\n",
    "\n",
    "### Step 2: Only article-looking links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f856b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_2 = [link for link in step_1 if link.startswith(\"/wiki/\")]\n",
    "print(f\"Step 2: Only /wiki/ links: {len(step_2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d24af3",
   "metadata": {},
   "source": [
    "**Take note**\n",
    "\n",
    "This is better, but still includes special pages and internal anchors.\n",
    "\n",
    "### Step 3: Remove special pages and anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4ae15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_3 = [link for link in step_2 if \":\" not in link and \"#\" not in link]\n",
    "print(f\"Step 3: Remove non-article links: {len(step_3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fe09d4",
   "metadata": {},
   "source": [
    "**Take note**\n",
    "\n",
    "We now have a cleaner list, but we can still be more precise.\n",
    "\n",
    "### Step 4: Limit to the list content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7576a741",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_4 = response.css(\"div.mw-parser-output ul li a::attr(href)\").getall()\n",
    "step_4 = [link for link in step_4 if link.startswith(\"/wiki/\") and \":\" not in link and \"#\" not in link]\n",
    "print(f\"Step 4: Links inside lists: {len(step_4)}\")\n",
    "print(\"Sample links:\", step_4[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314c6804",
   "metadata": {},
   "source": [
    "**Take note**\n",
    "\n",
    "This is the first selector that matches our actual intention.\n",
    "\n",
    "### CSS vs XPath\n",
    "\n",
    "CSS selectors are shorter and easier to read for most HTML tasks.\n",
    "\n",
    "XPath is more powerful for navigation and complex conditions.\n",
    "\n",
    "Example CSS:\n",
    "\n",
    "```css\n",
    "div.mw-parser-output ul li a::attr(href)\n",
    "```\n",
    "\n",
    "Example XPath:\n",
    "\n",
    "```xpath\n",
    "//div[contains(@class, \"mw-parser-output\")]//ul//li//a/@href\n",
    "```\n",
    "\n",
    "In this course, start with CSS. Use XPath only when CSS is awkward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e83605",
   "metadata": {},
   "source": [
    "## Section 4: Known Target Page and `read_html`\n",
    "\n",
    "Assume we already know the page we want.\n",
    "\n",
    "**Why `read_html`?**\n",
    "\n",
    "It scans the HTML and tries to detect tables automatically. This saves time when the table is well structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1889a3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOOD_URL = \"https://en.wikipedia.org/wiki/Orange_juice\"\n",
    "\n",
    "food_response = requests.get(FOOD_URL, headers=headers, timeout=30)\n",
    "food_html = food_response.text\n",
    "\n",
    "tables = pd.read_html(StringIO(food_html))\n",
    "print(f\"Tables found: {len(tables)}\")\n",
    "\n",
    "table = tables[0] if tables else pd.DataFrame()\n",
    "print(table.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d7db3a",
   "metadata": {},
   "source": [
    "**Take note**\n",
    "\n",
    "`read_html` returns a list of tables. You choose the one you want.\n",
    "\n",
    "### Alternative: Manual Extraction Without `read_html`\n",
    "\n",
    "If `read_html` fails or you want full control, use selectors and navigate the `<table>` manually with its `<tr>` and `<th>` and `<td>` elements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c83c45",
   "metadata": {},
   "source": [
    "## Section 5: Move the Logic Into a Spider\n",
    "\n",
    "The notebook is for exploration. The spider is for repeatable collection.\n",
    "\n",
    "**Why move to a `.py` file?**\n",
    "\n",
    "A spider can be run from the terminal and reused later without rerunning every notebook cell.\n",
    "\n",
    "<details class=\"special\">\n",
    "<summary>Click to view the spider code</summary>\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class WikipediaFoodSpider(scrapy.Spider):\n",
    "    name = \"wikipedia_food\"\n",
    "    start_urls = [\"https://en.wikipedia.org/wiki/List_of_foods\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        title = response.css(\"h1 span::text\").get()\n",
    "        image_urls = response.css(\"img::attr(src)\").getall()\n",
    "        image_urls = [\n",
    "            url if url.startswith(\"http\") else f\"https://en.wikipedia.org{url}\"\n",
    "            for url in image_urls\n",
    "        ]\n",
    "\n",
    "        yield {\n",
    "            \"title\": title,\n",
    "            \"image_urls\": image_urls[:5]\n",
    "        }\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "**How to run the spider**\n",
    "\n",
    "1. Save the code above to `w02_wikipedia_spider.py`\n",
    "2. Run it from the terminal:\n",
    "\n",
    "```bash\n",
    "scrapy runspider w02_wikipedia_spider.py -O w02-food.jsonl\n",
    "```\n",
    "\n",
    "The output file is JSON Lines, one record per line. This is good for streaming data and large files.\n",
    "\n",
    "**Take Note:** learn more about the parameters you can pass to `runspider` by running: `scrapy runspider --help`"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "food",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
